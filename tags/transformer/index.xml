<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>transformer on Jonathan`s Blog</title>
    <link>http://jonathanwayy.xyz/tags/transformer/</link>
    <description>Recent content in transformer on Jonathan`s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>All rights reserved - 2020</copyright>
    <lastBuildDate>Sat, 10 Jul 2021 11:14:29 +0800</lastBuildDate><atom:link href="http://jonathanwayy.xyz/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[论文阅读笔记 -- ViT / 跨模态检索] Fine-grained Visual Textual Alignment (2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn37/</link>
      <pubDate>Sat, 10 Jul 2021 11:14:29 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn37/</guid>
      <description>2008.05231 Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders (2020) 背景 在分类任务上预训练的 CNN 网络所提取的特征通常只能捕捉到图像的全局描述，而忽视了重要的局部细节。 现有方法的问题 由于交</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 多模态预训练] Unicoder-VL (AAAI 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn36/</link>
      <pubDate>Sat, 10 Jul 2021 10:00:50 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn36/</guid>
      <description>Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training (AAAI 2020) 背景 尚未出现能够够直接处理跨模态任务与数据的预训练模型。 本文基于多层 Transformer 提出一种通用视觉语言编码器 (Universal Encoder for Vision And Language, Uni</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] Swin Transformer (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn35/</link>
      <pubDate>Fri, 09 Jul 2021 10:54:19 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn35/</guid>
      <description>2103.14030 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (2021) 开源代码传送门 背景 Transformer 在视觉任务上的主要困难 视觉元素的尺度可能相当不同，但是当前工作中 token 都固定尺度 图像具有更高的像素分辨率</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] XCiT: Cross-Covariance Image Transformers (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn32/</link>
      <pubDate>Mon, 05 Jul 2021 12:59:51 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn32/</guid>
      <description>2106.09681 XCiT: Cross-Covariance Image Transformers (2021) 开源代码传送门 背景 Transformers 中自注意力模块计算复杂度高。 本文用一种转置的注意力 (transposed attention) 取代自注意力，称为交叉协方差注意力 (cross-covariance attention, XCA)，其对于</description>
    </item>
    
  </channel>
</rss>
